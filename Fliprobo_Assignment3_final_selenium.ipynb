{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2f37f2d",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "\n",
    "First get the webpage https://www.naukri.com/\n",
    "Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "Then click the search button.\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data. Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdc495c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data extraction\n",
    "def job_records():\n",
    "    df=pd.DataFrame(columns=['job-title', 'job-location', 'company_name', 'experience_required'])\n",
    "    titles= driver.find_elements_by_xpath('//div[@class=\"info fleft\"]/a')  \n",
    "\n",
    "    company_names=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "\n",
    "    job_locations=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "\n",
    "\n",
    "    experience_require=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "    for i in range(0,10):\n",
    "        df=df.append({'job-title':titles[i].text,'job-location':job_locations[i].text,\n",
    "                 'company_name':company_names[i].text,'experience_required':experience_require[i].text},ignore_index=True)\n",
    "        time.sleep(1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6af2d6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first connect to the webdriver\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "#connection after uploading driver in jupyter notebook\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "\n",
    "url=\" https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "search_job=driver.find_element_by_class_name(\"suggestor-input \")\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "#locating ele by full xpath\n",
    "search_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "search_location\n",
    "search_location.send_keys(\"Bangalore\")\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "record=job_records()\n",
    "record"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5413a959",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdceeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first connect to the webdriver\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "#connection after uploading driver in jupyter notebook\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "\n",
    "url= \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "#define dataframe\n",
    "df=pd.DataFrame(columns=['job-title', 'job-location', 'company_name', 'experience_required'])\n",
    "\n",
    "#entering job title\n",
    "search_job=driver.find_element_by_class_name(\"suggestor-input \")\n",
    "search_job.send_keys('Data Scientist')\n",
    "\n",
    "#locating ele by full xpath\n",
    "search_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "search_location\n",
    "#sending location\n",
    "search_location.send_keys('Bangalore')\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2b68f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job-title</th>\n",
       "      <th>job-location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Uber</td>\n",
       "      <td>6-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HCL Tech Opening - Lead Data Scientist</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Chennai...</td>\n",
       "      <td>HCL</td>\n",
       "      <td>10-20 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist (R Programming)</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Ignitho</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist/Senior Data Scientist - Python</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>ApicalGo Consultancy</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist - Python/Machine Learnin...</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Bangalor...</td>\n",
       "      <td>Altimax Business Solutions</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sr . Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Visa</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need Data scientists and data engineers - WFH-...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, M...</td>\n",
       "      <td>Covalense Technologies Private Limited</td>\n",
       "      <td>8-13 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AirSeva</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data scientist- ML/python @ Unitedh...</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Mobile Premier League</td>\n",
       "      <td>8-12 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job-title  \\\n",
       "0                                  Sr Data Scientist   \n",
       "1             HCL Tech Opening - Lead Data Scientist   \n",
       "2              Senior Data Scientist (R Programming)   \n",
       "3      Data Scientist/Senior Data Scientist - Python   \n",
       "4  Senior Data Scientist - Python/Machine Learnin...   \n",
       "5                                Sr . Data Scientist   \n",
       "6  Need Data scientists and data engineers - WFH-...   \n",
       "7                                     Data Scientist   \n",
       "8  Hiring For Data scientist- ML/python @ Unitedh...   \n",
       "9                           Principal Data Scientist   \n",
       "\n",
       "                                        job-location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1  Kolkata, Hyderabad/Secunderabad, Pune, Chennai...   \n",
       "2                                             Remote   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Mumbai, Hyderabad/Secunderabad, Pune, Bangalor...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, M...   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                             company_name experience_required  \n",
       "0                                    Uber            6-12 Yrs  \n",
       "1                                     HCL           10-20 Yrs  \n",
       "2                                 Ignitho             4-9 Yrs  \n",
       "3                    ApicalGo Consultancy             3-8 Yrs  \n",
       "4              Altimax Business Solutions             4-9 Yrs  \n",
       "5                                    Visa             6-8 Yrs  \n",
       "6  Covalense Technologies Private Limited            8-13 Yrs  \n",
       "7                                 AirSeva            6-11 Yrs  \n",
       "8                                   Optum             2-5 Yrs  \n",
       "9                   Mobile Premier League            8-12 Yrs  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record=job_records()\n",
    "record"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1aeb917d",
   "metadata": {},
   "source": [
    "Q3:You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf3b4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n",
    "#define dataframe\n",
    "df=pd.DataFrame(columns=['job-title', 'job-location', 'company_name', 'experience_required'])\n",
    "search_job= driver.find_element_by_xpath('//input[@class=\"suggestor-input \"][1]')\n",
    "search_job.send_keys('Data Scientist')\n",
    "search_submit= driver.find_element_by_xpath('//div[@class=\"qsbSubmit\"]')\n",
    "search_submit.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#location filter\n",
    "chk_delhi=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/i')\n",
    "chk_delhi.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "955ed32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sal filter\n",
    "chk_sal=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/i')\n",
    "chk_sal.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a0baffe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job-title</th>\n",
       "      <th>job-location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent Opportunity For Freshers For AI/ML, ...</td>\n",
       "      <td>Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...</td>\n",
       "      <td>NTT Data</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst / Data Scientist / Business Analy...</td>\n",
       "      <td>Noida, New Delhi, Delhi / NCR</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For Senior Data Scientist-Noida</td>\n",
       "      <td>Noida, New Delhi, Greater Noida</td>\n",
       "      <td>Lumiq.ai</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data scientist- Python</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>TeamPlus Staffing Solution Pvt Ltd</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist _NLP</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR\\n(WFH during ...</td>\n",
       "      <td>EXL</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist (freelance)</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "      <td>2Coms</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Only Fresher / Python Data Scientist / Trainee...</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Delhi / NCR\\n(WFH during Covid)</td>\n",
       "      <td>Indihire HR Consultants Private Limited</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job-title  \\\n",
       "0  Excellent Opportunity For Freshers For AI/ML, ...   \n",
       "1  Data Analyst / Data Scientist / Business Analy...   \n",
       "2             Hiring For Senior Data Scientist-Noida   \n",
       "3                                     Data Scientist   \n",
       "4                             Data scientist- Python   \n",
       "5                                Data Scientist _NLP   \n",
       "6                         Data Scientist (freelance)   \n",
       "7                     Data Scientist - MIND Infotech   \n",
       "8  Only Fresher / Python Data Scientist / Trainee...   \n",
       "9                                Lead Data Scientist   \n",
       "\n",
       "                                        job-location  \\\n",
       "0  Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...   \n",
       "1                      Noida, New Delhi, Delhi / NCR   \n",
       "2                    Noida, New Delhi, Greater Noida   \n",
       "3  Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5  Bangalore/Bengaluru, Delhi / NCR\\n(WFH during ...   \n",
       "6                                   New Delhi, Delhi   \n",
       "7                                              Noida   \n",
       "8                 Noida, New Delhi, Gurgaon/Gurugram   \n",
       "9                    Delhi / NCR\\n(WFH during Covid)   \n",
       "\n",
       "                               company_name experience_required  \n",
       "0                                  NTT Data             0-0 Yrs  \n",
       "1                 GABA Consultancy services             0-0 Yrs  \n",
       "2                                  Lumiq.ai             2-6 Yrs  \n",
       "3   Mount Talent Consulting Private Limited             1-4 Yrs  \n",
       "4        TeamPlus Staffing Solution Pvt Ltd             3-6 Yrs  \n",
       "5                                       EXL             3-8 Yrs  \n",
       "6                                     2Coms             2-7 Yrs  \n",
       "7  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED             4-8 Yrs  \n",
       "8                 GABA Consultancy services             0-0 Yrs  \n",
       "9   Indihire HR Consultants Private Limited             2-4 Yrs  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "record=job_records()\n",
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0db259",
   "metadata": {},
   "source": [
    "**Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:**\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "729ec2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the webdriver\n",
    "driver= webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "#connection after uploading driver in jupyter notebook\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4a4976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\" https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)\n",
    "close_add=driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "close_add.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb454292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search \"sunglasses\"\n",
    "search_result=driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_result.send_keys(\"sunglasses\")\n",
    "search = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca963cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arnette</td>\n",
       "      <td>Mirrored Retro Square Sunglasses (56)</td>\n",
       "      <td>₹2,549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Aviator S...</td>\n",
       "      <td>₹980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>₹253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection Aviator Sunglasses (57)</td>\n",
       "      <td>₹759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (52)</td>\n",
       "      <td>₹639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wrap-around Sunglasses (Free Size)</td>\n",
       "      <td>₹929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>₹250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection Aviator Sunglasses (57)</td>\n",
       "      <td>₹284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                        Description  Prices\n",
       "0         Arnette              Mirrored Retro Square Sunglasses (56)  ₹2,549\n",
       "1   VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...    ₹980\n",
       "2            SRPM             UV Protection Wayfarer Sunglasses (50)    ₹198\n",
       "3          SUNBEE  UV Protection, Polarized Wayfarer Sunglasses (...    ₹253\n",
       "4          PIRASO              UV Protection Aviator Sunglasses (54)    ₹229\n",
       "..            ...                                                ...     ...\n",
       "95      ROYAL SON   Polarized, UV Protection Aviator Sunglasses (57)    ₹759\n",
       "96       Fastrack                UV Protection Round Sunglasses (52)    ₹639\n",
       "97       Fastrack   UV Protection Wrap-around Sunglasses (Free Size)    ₹929\n",
       "98         PIRASO              UV Protection Aviator Sunglasses (55)    ₹250\n",
       "99         GANSTA              UV Protection Aviator Sunglasses (57)    ₹284\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.DataFrame(columns=['Brand','Description','Prices'])\n",
    "for j in range(1,4):\n",
    "    url=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(j)\n",
    "    driver.get(url)\n",
    "    title= driver.find_elements_by_xpath('//div[@class=\"_2B099V\"]/div[1]')\n",
    "    desc= driver.find_elements_by_xpath('//div[@class=\"_2B099V\"]/a[1]')\n",
    "    prices= driver.find_elements_by_xpath('//div[@class=\"_2B099V\"]/a[2]/div[1]/div[1]')\n",
    "    time.sleep(1)\n",
    "   \n",
    "    for i in range(0,len(title)):\n",
    "        df1 = df1.append({'Brand':title[i].text,'Description':desc[i].text ,'Prices':prices[i].text},ignore_index=True)\n",
    "    time.sleep(2)\n",
    "    \n",
    "time.sleep(1)\n",
    "driver.close()\n",
    "df1.head(100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99fbf7f5",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "df=pd.DataFrame(columns=['Rating','Review_summary','Full_review'])\n",
    "\n",
    "for j in range(1,11):\n",
    "    url1='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3IXQLM&marketplace=FLIPKART&page='+str(j)\n",
    "    driver.get(url1)\n",
    "    gride=driver.find_elements_by_xpath('//div[@class=\"_1YokD2 _3Mn1Gg col-9-12\"]/div')[2:12]\n",
    "    \n",
    "\n",
    "    for i in gride:\n",
    "        Ratings =i.find_element_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "        #print(Ratings.text)\n",
    "        Rating.append(Ratings.text)\n",
    "\n",
    "        Reviews_summary =i.find_element_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "        #print(Reviews_summary.text)\n",
    "        Review_summary.append(Reviews_summary.text)\n",
    "\n",
    "        Full_reviews =i.find_element_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "        #print(Full_reviews.text)\n",
    "        Full_review.append(Full_reviews.text)\n",
    "        df=df.append({'Rating':Ratings.text,'Review_summary':Reviews_summary.text,\n",
    "                          'Full_review':Full_reviews.text},ignore_index=True)\n",
    "        time.sleep(1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce293d",
   "metadata": {},
   "source": [
    "**Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the\n",
    "search field.**\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver= webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "#connection after uploading driver in jupyter notebook\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=\"https://www.flipkart.com\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "close_log=driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "close_log.click()\n",
    "\n",
    "search_result=driver.find_element_by_class_name(\"_3704LK\")\n",
    "search_result.send_keys(\"sneakers\")\n",
    "search = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['Brand','Description','Rate','Discount'])\n",
    "for j in range(1,4):\n",
    "    url=\"https://www.flipkart.com/search?q=sneakers&as=on&as-show=on&otracker=AS_Query_OrganicAutoSuggest_3_2_na_na_na&otracker1=AS_Query_OrganicAutoSuggest_3_2_na_na_na&as-pos=3&as-type=RECENT&suggestionId=sneakers&requestId=d635f1c2-ae40-403b-9b9b-24dfec7c4a88&as-searchtext=sn&page=\"+str(j)\n",
    "    #driver.get(url)  \n",
    "    driver.get(url) \n",
    "    grid=driver.find_elements_by_xpath('//div[@class=\"_2B099V\"]')    \n",
    "    for i in grid:\n",
    "        brand=i.find_element_by_tag_name('div')\n",
    "\n",
    "        dis=i.find_elements_by_tag_name('a')[0]\n",
    "        #to find rate\n",
    "        r=i.find_elements_by_tag_name('a')[1]\n",
    "        p=r.find_element_by_tag_name('div')\n",
    "        rate=p.find_elements_by_tag_name('div')[0]\n",
    "        discount=p.find_elements_by_tag_name('div')[-1]\n",
    "        df=df.append({'Brand':brand.text,'Description':dis.text,\n",
    "                      'Rate':rate.text,'Discount':discount.text},ignore_index=True)\n",
    "        time.sleep(1)\n",
    "time.sleep(1)\n",
    "driver.close()\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b47c4dcc",
   "metadata": {},
   "source": [
    "**Q7Go to the link - https://www.myntra.com/shoes**\n",
    "Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”, as shown inthe below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply price filter\n",
    "price_filter=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "print(price_filter.click())\n",
    "\n",
    "#apply color filter\n",
    "color_filter=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "print(color_filter.click())\n",
    "\n",
    "df=pd.DataFrame(columns=['Brand','Discription','Price'])\n",
    "\n",
    "#to fetch product details from 2 pages forloop\n",
    "\n",
    "for p in range(1,3):\n",
    "    print(p)\n",
    "    print(url)\n",
    "    \n",
    "    product=driver.find_elements_by_xpath('//div[@class=\"product-productMetaInfo\"]')\n",
    "    print(len(product))\n",
    "    for i in product:\n",
    "       \n",
    "        Brand=i.find_element_by_tag_name('h3')\n",
    "        print(Brand.text)\n",
    "        Discription=i.find_element_by_tag_name('h4')\n",
    "        print(Discription.text)\n",
    "        \n",
    "        price1=i.find_element_by_xpath('div')\n",
    "        price=price1.find_element_by_tag_name('span').text\n",
    "        # spliting discounted price and actual price\n",
    "        prices=price.split()\n",
    "        df=df.append({'Brand':Brand.text,'Discription':Discription.text,'Price':prices[1]},ignore_index=True)\n",
    "         \n",
    "    if(p==1):\n",
    "        url=\"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&p=\"+str(p+1)+\"&rf=Price%3A7134.0_14089.0_7134.0%20TO%2014089.0\"\n",
    "        print(url)\n",
    "        driver.get(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945dc61",
   "metadata": {},
   "source": [
    "**Q8: Go to webpage https://www.amazon.in/**\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url='https://www.amazon.in/ '\n",
    "driver.get(url)\n",
    "\n",
    "df_i7=pd.DataFrame(columns=['title','price','rating'])\n",
    "search = driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search.send_keys('Laptop')\n",
    "\n",
    "submit=driver.find_element_by_xpath('//input[@id=\"nav-search-submit-button\"]')\n",
    "submit.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffe39a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpu_i7=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[12]/span/a/div/label/i')\n",
    "print(cpu_i7.text)\n",
    "cpu_i7.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]')\n",
    "for i in x:\n",
    "    title=i.find_element_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "   \n",
    "    price=i.find_element_by_xpath('//span[@class=\"a-price-whole\"]')\n",
    "\n",
    "    rating=i.find_element_by_xpath('//div[@class=\"a-section a-spacing-none a-spacing-top-micro\"]/div/span/span/a/i/span')\n",
    "\n",
    "    df_i7=df_i7.append({'title':title.text,'price':price.text,'rating':rating.text},ignore_index=True)\n",
    "print(\"***Records of Intel Core i7****\")     \n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8c4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clear_filter=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[1]/span/a/span[2]')\n",
    "clear_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_i9=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[13]/span/a/div/label/i')\n",
    "print(cpu_i9.click())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ccb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i9=pd.DataFrame(columns=['title','price','rating'])\n",
    "x=driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]')\n",
    "for i in x:\n",
    "    title=i.find_element_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "   \n",
    "    price=i.find_element_by_xpath('//span[@class=\"a-price-whole\"]')\n",
    "\n",
    "    rating=i.find_element_by_xpath('//div[@class=\"a-section a-spacing-none a-spacing-top-micro\"]/div/span/span/a/i/span')\n",
    "\n",
    "    df_i9=df_i9.append({'title':title.text,'price':price.text,'rating':rating.text},ignore_index=True)\n",
    "print(\"***Records of Intel Core i9****\")    \n",
    "print(df_i9.head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74dd00c9",
   "metadata": {},
   "source": [
    "**Q9:Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:**\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companie, Skills” enter “Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url='https://www.ambitionbox.com/'\n",
    "driver.get(url)\n",
    "job=driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[6]')\n",
    "job.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div/div/div/div/span/input')\n",
    "job_des.send_keys('Data Scientist')\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div/div/div/button/span')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95543ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loctaion=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[2]/div[1]/div/div/div/div[2]/div[1]/i')\n",
    "loctaion.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67759a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_name=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input')\n",
    "loc_name.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#radio button selection\n",
    "rd_loc=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "rd_loc.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=driver.find_elements_by_xpath('//div[@itemprop=\"itemListElement\"]')\n",
    "x=0\n",
    "df=pd.DataFrame(columns=['Job_Title','company_name','Rating','Package/exp','Locations','Skill'])\n",
    "\n",
    "company_name=[]\n",
    "title=[]\n",
    "Rating=[]\n",
    "package=[]\n",
    "locations=[]\n",
    "skill=[]\n",
    "for i in grid:\n",
    "    x=x+1\n",
    "    print(x)\n",
    "    \n",
    "    titles=i.find_element_by_tag_name('h2')\n",
    "    title.append(titles.text)\n",
    "    \n",
    "    company_names=i.find_element_by_tag_name('p')\n",
    "    company_name.append(company_names.text)\n",
    "    \n",
    "    ratings=i.find_element_by_tag_name('span')\n",
    "    Rating.append(ratings.text)\n",
    "    \n",
    "   \n",
    "basic_info=driver.find_elements_by_xpath('//div[@class=\"job-basic-info show-flex\"]')\n",
    "\n",
    "for j in basic_info[1:]:\n",
    "    info=j.find_elements_by_tag_name('p')\n",
    "\n",
    "    package.append(info[0].text)\n",
    "    if(info[1].text=='Not Disclosed'):\n",
    "        locations.append(info[2].text)\n",
    "        skill.append(info[3].text)\n",
    "\n",
    "    else:\n",
    "        locations.append(info[1].text)\n",
    "        skill.append(info[2].text)\n",
    "\n",
    "#df=df2.append({'Package':package,'Locations':locations,'Skill':skill},ignore_index=True)\n",
    "df['Package/exp']=package\n",
    "df['Locations']=locations\n",
    "df['Skill']=skill\n",
    "df['Job_Title']=title\n",
    "df['company_name']=company_name\n",
    "df['Rating']=Rating\n",
    "\n",
    "print(df.head(10))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7565a727",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "ASSIGNMENT 2\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe.\n",
    "Note: All the steps required during scraping should be done through code only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5792de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url='https://www.ambitionbox.com/'\n",
    "driver.get(url)\n",
    "job=driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[4]')\n",
    "job.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div/div/div/div[1]/span/input')\n",
    "job_des.send_keys('Data Scientist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c33787",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\nbson\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url='https://www.ambitionbox.com/profile/data-scientist-salary'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d05f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name=[]\n",
    "total_sal_records=[]\n",
    "avg_sal=[]\n",
    "min_sal=[]\n",
    "max_sal=[]\n",
    "exp_required=[]\n",
    "df=pd.DataFrame(columns=['Company name', 'Total salary record', 'Average salary', 'Minsalary', 'Max Salary','experience required'])\n",
    "records=driver.find_elements_by_xpath('//div[@class=\"result-row\"]')\n",
    "\n",
    "\n",
    "name=driver.find_elements_by_xpath('//div[@class=\"name\"]')\n",
    "for j in name:\n",
    "        comp_name=j.find_element_by_tag_name('a')\n",
    "        company_name.append(comp_name.text)\n",
    "        total_sal=j.find_element_by_tag_name('span')\n",
    "        total_sal_records.append(total_sal.text)\n",
    "exp=driver.find_elements_by_xpath('//div[@class=\"salaries one-line sbold-list-header\"]/span')\n",
    "\n",
    "for y in sal_records:\n",
    "    avgsal=y.text.split() \n",
    "    avg_sal.append(avgsal[1])\n",
    "    min_sal.append(avgsal[3])\n",
    "    max_sal.append(avgsal[5])\n",
    "    \n",
    "for i in exp:\n",
    "    exp_required.append(i.text)       \n",
    "\n",
    "df['Company name']=avg_sal\n",
    "df['Total salary record']=min_sal\n",
    "df['Average salary']=max_sal\n",
    "df['Minsalary']=company_name\n",
    "df['Max Salary']=total_sal_records\n",
    "df['experience required']=exp_required\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
